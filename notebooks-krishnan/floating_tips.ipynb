{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1395c899-1da5-44ca-84dc-8de289df9c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:43.091933Z",
     "iopub.status.busy": "2022-06-10T23:30:43.091634Z",
     "iopub.status.idle": "2022-06-10T23:30:43.104222Z",
     "shell.execute_reply": "2022-06-10T23:30:43.103560Z",
     "shell.execute_reply.started": "2022-06-10T23:30:43.091849Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d397b9-8794-4962-9b91-3f87c7275e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:43.105417Z",
     "iopub.status.busy": "2022-06-10T23:30:43.105259Z",
     "iopub.status.idle": "2022-06-10T23:30:46.342902Z",
     "shell.execute_reply": "2022-06-10T23:30:46.342284Z",
     "shell.execute_reply.started": "2022-06-10T23:30:43.105396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_37' (/scr-ssd/ksrini/Downloads/isaacgym-3/python/isaacgym/_bindings/linux-x86_64/gym_37.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /scr-ssd/ksrini/Downloads/isaacgym-3/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 1.11.0+cu113\n",
      "Device count 1\n",
      "/scr-ssd/ksrini/Downloads/isaacgym-3/python/isaacgym/_bindings/src/gymtorch\n",
      "Using /afs/cs.stanford.edu/u/ksrini/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
      "Emitting ninja build file /afs/cs.stanford.edu/u/ksrini/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module gymtorch...\n",
      "WARNING: Unable to import pinocchio, skipping import\n",
      "WARNING: Unable to import pinocchio, skipping import\n",
      "WARNING: Unable to import pinocchio, skipping import\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Any\n",
    "from isaacgym import gymapi, gymtorch\n",
    "from nerf_grasping.grasp_opt import grasp_matrix, rot_from_vec\n",
    "from nerf_grasping.control import pos_control, force_opt\n",
    "from nerf_grasping import grasp_utils\n",
    "from nerf_grasping.quaternions import Quaternion\n",
    "from nerf_grasping.sim import ig_utils\n",
    "from nerf_grasping.sim import ig_objects\n",
    "from nerf_grasping.sim import ig_viz_utils\n",
    "from nerf_grasping.sim.ig_robot import FingertipRobot\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(os.path.abspath(\"./\")).parents[0]\n",
    "asset_dir = f\"{root_dir}/assets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4eecbf-af1a-470e-a916-d06d45fda20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T22:42:42.811800Z",
     "iopub.status.busy": "2022-06-08T22:42:42.811349Z",
     "iopub.status.idle": "2022-06-08T22:42:42.817168Z",
     "shell.execute_reply": "2022-06-08T22:42:42.815875Z",
     "shell.execute_reply.started": "2022-06-08T22:42:42.811742Z"
    },
    "tags": []
   },
   "source": [
    "## IG helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd91cef-f66d-411c-8056-16678aa9d3f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:46.344059Z",
     "iopub.status.busy": "2022-06-10T23:30:46.343914Z",
     "iopub.status.idle": "2022-06-10T23:30:46.374675Z",
     "shell.execute_reply": "2022-06-10T23:30:46.374076Z",
     "shell.execute_reply.started": "2022-06-10T23:30:46.344039Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_viewer():\n",
    "    viewer = gym.create_viewer(sim, gymapi.CameraProperties())\n",
    "    # position outside stage\n",
    "    cam_pos = gymapi.Vec3(0.7, 0.175, 0.6)\n",
    "    # position above banana\n",
    "    cam_pos = gymapi.Vec3(0.1, 0.02, 0.4)\n",
    "    cam_target = gymapi.Vec3(0, 0, 0.2)\n",
    "    gym.viewer_camera_look_at(viewer, env, cam_pos, cam_target)\n",
    "    return viewer\n",
    "\n",
    "\n",
    "def step_gym():\n",
    "    gym.simulate(sim)\n",
    "    gym.fetch_results(sim, True)\n",
    "\n",
    "    gym.step_graphics(sim)\n",
    "    if viewer is not None:\n",
    "        gym.draw_viewer(viewer, sim, True)\n",
    "        gym.sync_frame_time(sim)\n",
    "    refresh_tensors()\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    plane_params = gymapi.PlaneParams()\n",
    "    plane_params.normal = gymapi.Vec3(0, 0, 1)  # z-up!\n",
    "    gym.add_ground(sim, plane_params)\n",
    "\n",
    "    spacing = 1.0\n",
    "    env_lower = gymapi.Vec3(-spacing, -spacing, 0.0)\n",
    "    env_upper = gymapi.Vec3(spacing, spacing, spacing)\n",
    "    env = gym.create_env(sim, env_lower, env_upper, 0)\n",
    "    return env\n",
    "\n",
    "\n",
    "def refresh_tensors():\n",
    "    gym.refresh_mass_matrix_tensors(sim)\n",
    "    gym.refresh_jacobian_tensors(sim)\n",
    "    gym.refresh_dof_state_tensor(sim)\n",
    "    gym.refresh_rigid_body_state_tensor(sim)\n",
    "\n",
    "\n",
    "def setup_sim():\n",
    "    args = ig_utils.parse_arguments(description=\"Trifinger test\")\n",
    "    # only tested with this one\n",
    "    assert args.physics_engine == gymapi.SIM_PHYSX\n",
    "\n",
    "    # configure sim\n",
    "    sim_params = gymapi.SimParams()\n",
    "    sim_params.dt = 1.0 / 60.0\n",
    "\n",
    "    sim_params.up_axis = gymapi.UP_AXIS_Z\n",
    "    sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.8)\n",
    "\n",
    "    sim_params.physx.solver_type = 1\n",
    "    sim_params.physx.num_position_iterations = 6\n",
    "    sim_params.physx.num_velocity_iterations = 0\n",
    "    sim_params.physx.num_threads = args.num_threads\n",
    "    sim_params.physx.use_gpu = args.use_gpu\n",
    "    # sim_params.physx.use_gpu = True\n",
    "\n",
    "    # sim_params.use_gpu_pipeline = True\n",
    "    sim_params.use_gpu_pipeline = False\n",
    "    sim = gym.create_sim(\n",
    "        args.compute_device_id,\n",
    "        args.graphics_device_id,\n",
    "        args.physics_engine,\n",
    "        sim_params,\n",
    "    )\n",
    "    assert sim is not None\n",
    "\n",
    "    # intensity = 0.01 # for nerf generation\n",
    "    # ambient = 0.21 / intensity\n",
    "    intensity = 0.5\n",
    "    ambient = 0.10 / intensity\n",
    "    intensity = gymapi.Vec3(intensity, intensity, intensity)\n",
    "    ambient = gymapi.Vec3(ambient, ambient, ambient)\n",
    "\n",
    "    gym.set_light_parameters(sim, 0, intensity, ambient, gymapi.Vec3(0.5, 1, 1))\n",
    "    gym.set_light_parameters(sim, 1, intensity, ambient, gymapi.Vec3(1, 0, 1))\n",
    "    gym.set_light_parameters(sim, 2, intensity, ambient,\n",
    "                             gymapi.Vec3(0.5, -1, 1))\n",
    "    gym.set_light_parameters(sim, 3, intensity, ambient, gymapi.Vec3(0, 0, 1))\n",
    "    return sim\n",
    "\n",
    "\n",
    "def setup_stage(env):\n",
    "    # this one is convex decomposed\n",
    "    stage_urdf_file = \"trifinger/robot_properties_fingers/urdf/high_table_boundary.urdf\"\n",
    "    # stage_urdf_file = \"trifinger/robot_properties_fingers/urdf/trifinger_stage.urdf\"\n",
    "    # stage_urdf_file = \"trifinger/robot_properties_fingers/urdf/stage.urdf\"\n",
    "\n",
    "    asset_options = gymapi.AssetOptions()\n",
    "    asset_options.disable_gravity = True\n",
    "    asset_options.fix_base_link = True\n",
    "    asset_options.flip_visual_attachments = False\n",
    "    asset_options.use_mesh_materials = True\n",
    "    asset_options.thickness = 0.001\n",
    "\n",
    "    stage_asset = gym.load_asset(sim, asset_dir, stage_urdf_file, asset_options)\n",
    "    gym.create_actor(env,\n",
    "                     stage_asset,\n",
    "                     gymapi.Transform(),\n",
    "                     \"Stage\",\n",
    "                     0,\n",
    "                     0,\n",
    "                     segmentationId=1)\n",
    "\n",
    "\n",
    "def get_mesh_contacts(gt_mesh,\n",
    "                      grasp_points,\n",
    "                      pos_offset=None,\n",
    "                      rot_offset=None,\n",
    "                      return_dist=False):\n",
    "    if pos_offset is not None:\n",
    "        # project grasp_points into object frame\n",
    "        grasp_points -= pos_offset\n",
    "        grasp_points = np.stack([rot_offset.rotate(gp) for gp in grasp_points])\n",
    "    points, distance, index = trimesh.proximity.closest_point(\n",
    "        gt_mesh, grasp_points)\n",
    "    # grasp normals follow convention that points into surface,\n",
    "    # trimesh computes normals pointing out of surface\n",
    "    grasp_normals = -gt_mesh.face_normals[index]\n",
    "    if pos_offset is not None:\n",
    "        # project back into world frame\n",
    "        points += pos_offset\n",
    "        grasp_normals = np.stack(\n",
    "            [rot_offset.T.rotate(x) for x in grasp_normals])\n",
    "    retval = ((points, grasp_normals) if not return_dist else\n",
    "              (points, grasp_normals, distance))\n",
    "    return retval\n",
    "\n",
    "\n",
    "def random_forces(timestep):\n",
    "    fx = -np.sin(timestep * np.pi / 10) * 0.025 + 0.001\n",
    "    fy = -np.sin(timestep * np.pi / 5) * 0.025 + 0.001\n",
    "    f = np.array([[fx, fy, 0.0]] * 3)\n",
    "    return f\n",
    "\n",
    "\n",
    "def closest_point(a, b, p):\n",
    "    ap = p - a\n",
    "    ab = b - a\n",
    "    res = []\n",
    "    for i in range(3):\n",
    "        result = a[i] + torch.dot(ap[i], ab[i]) / torch.dot(ab[i],\n",
    "                                                            ab[i]) * ab[i]\n",
    "        res.append(result)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9f9ab-41fe-4383-a245-575791bf9f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T07:37:50.304157Z",
     "iopub.status.busy": "2022-06-10T07:37:50.303696Z",
     "iopub.status.idle": "2022-06-10T07:37:50.332647Z",
     "shell.execute_reply": "2022-06-10T07:37:50.331927Z",
     "shell.execute_reply.started": "2022-06-10T07:37:50.304098Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Michal's FOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57890b1-af57-42b3-8f2b-f97c54a3fb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:46.375611Z",
     "iopub.status.busy": "2022-06-10T23:30:46.375473Z",
     "iopub.status.idle": "2022-06-10T23:30:46.498554Z",
     "shell.execute_reply": "2022-06-10T23:30:46.497532Z",
     "shell.execute_reply.started": "2022-06-10T23:30:46.375592Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "\n",
    "def skew_matrix(vectors):\n",
    "    skew = np.zeros(vectors.shape[:-1] + (3, 3))\n",
    "\n",
    "    skew[..., 0, 1] = -vectors[..., 2]\n",
    "    skew[..., 1, 2] = -vectors[..., 0]\n",
    "    skew[..., 2, 0] = -vectors[..., 1]\n",
    "    skew[..., 1, 0] = vectors[..., 2]\n",
    "    skew[..., 2, 1] = vectors[..., 0]\n",
    "    skew[..., 0, 2] = vectors[..., 1]\n",
    "\n",
    "    return skew\n",
    "\n",
    "\n",
    "def example_rotation_transform(normals):\n",
    "    # hopefully no one will try grabing directly under or above\n",
    "    global_z_axis = np.array([0, 0, 1])\n",
    "\n",
    "    #  n,3, 1      3, 3                       n, 3, 1\n",
    "    local_x = skew_matrix(global_z_axis) @ normals[..., None]\n",
    "\n",
    "    #  n,3,1         n,3,3              n,3,1\n",
    "    local_y = skew_matrix(normals) @ local_x\n",
    "\n",
    "    local_x /= np.linalg.norm(local_x, keepdims=True, axis=-2)\n",
    "    local_y /= np.linalg.norm(local_y, keepdims=True, axis=-2)\n",
    "\n",
    "    rotations = np.stack([local_x, local_y, normals[..., None]], axis=-1)[...,\n",
    "                                                                          0, :]\n",
    "    return rotations\n",
    "\n",
    "\n",
    "def calculate_grip_forces(positions,\n",
    "                          normals,\n",
    "                          target_force,\n",
    "                          target_torque,\n",
    "                          target_normal=0.4,\n",
    "                          mu=0.5):\n",
    "    \"\"\"positions are relative to object CG if we want unbalanced torques\"\"\"\n",
    "\n",
    "    torch_input = type(positions) == torch.Tensor\n",
    "    if torch_input:\n",
    "        assert type(\n",
    "            normals) == torch.Tensor, \"numpy vs torch needs to be consistant\"\n",
    "        assert type(target_force\n",
    "                   ) == torch.Tensor, \"numpy vs torch needs to be consistant\"\n",
    "        assert (type(target_torque) == torch.Tensor\n",
    "               ), \"numpy vs torch needs to be consistant\"\n",
    "        positions = positions.numpy()\n",
    "        normals = normals.numpy()\n",
    "        target_force = target_force.numpy()\n",
    "        target_torque = target_torque.numpy()\n",
    "\n",
    "    n, _ = positions.shape\n",
    "    assert normals.shape == (n, 3)\n",
    "    assert target_force.shape == (3,)\n",
    "\n",
    "    F = cp.Variable((n, 3))\n",
    "    constraints = []\n",
    "\n",
    "    normals = normals / np.linalg.norm(normals, axis=-1, keepdims=True)\n",
    "\n",
    "    total_force = np.zeros((3))\n",
    "    total_torque = np.zeros((3))\n",
    "\n",
    "    Q = []\n",
    "    for pos, norm, f in zip(positions, normals, F):\n",
    "        q = example_rotation_transform(norm)\n",
    "        Q.append(q)\n",
    "\n",
    "        total_force += q @ f\n",
    "        total_torque += skew_matrix(pos) @ q @ f\n",
    "\n",
    "    constraints.append(total_force == target_force)\n",
    "    constraints.append(total_torque == target_torque)\n",
    "\n",
    "    friction_cone = cp.norm(F[:, :2], axis=1) <= mu * F[:, 2]\n",
    "    constraints.append(friction_cone)\n",
    "\n",
    "    force_magnitudes = cp.norm(F - np.array([[0.0, 0.0, target_normal]]),\n",
    "                               axis=1)\n",
    "    # friction_magnitudes = cp.norm(F[:,2], axis=1)\n",
    "    prob = cp.Problem(cp.Minimize(cp.max(force_magnitudes)), constraints)\n",
    "    prob.solve()\n",
    "\n",
    "    if F.value is None:\n",
    "        print(\"Failed to solve!\")\n",
    "        return torch.zeros(9)\n",
    "\n",
    "    global_forces = np.zeros_like(F.value)\n",
    "    for i in range(n):\n",
    "        global_forces[i, :] = Q[i] @ F.value[i, :]\n",
    "\n",
    "    if torch_input:\n",
    "        global_forces = torch.tensor(global_forces).float()\n",
    "\n",
    "    return global_forces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75d907-2e0e-4578-a7ce-4c906ae87d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T01:36:25.034099Z",
     "iopub.status.busy": "2022-06-10T01:36:25.033917Z"
    },
    "tags": []
   },
   "source": [
    "**Visualizing ideal + actual grasp normals and closest point**\n",
    "```python\n",
    "gt_points, gt_normals, dist = get_mesh_contacts(obj.gt_mesh, robot.position, return_dist=True)\n",
    "closest_points = closest_point(grasp_points, grasp_points + grasp_normals, robot.position)\n",
    "\n",
    "robot.reset_actor()\n",
    "obj.reset_actor()\n",
    "ig_viz_utils.visualize_grasp_normals(gym, viewer, env, robot.position, \n",
    "                                     grasp_normals, des_z_dist=dist, \n",
    "                                     colors=[[0.,1.,0.]]*3)\n",
    "ig_viz_utils.visualize_grasp_normals(gym, viewer, env, robot.position, \n",
    "                                     gt_normals.astype('float32'), des_z_dist=dist,\n",
    "                                     colors=[[1.,0.,0.]]*3)\n",
    "ig_viz_utils.visualize_markers(gym, env, sim, points)\n",
    "\n",
    "while True: step_gym()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edff1e-18df-444e-b603-2b9d69fdc7da",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-09T06:44:40.719346Z",
     "iopub.status.idle": "2022-06-09T06:44:40.719544Z",
     "shell.execute_reply": "2022-06-09T06:44:40.719446Z"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "gym.get_actor_dof_count(env, 3)\n",
    "\n",
    "for i in range(4):\n",
    "    print(gym.get_actor_name(env, i))\n",
    "    print(gym.get_sim_rigid_body_states(sim, gymapi.STATE_POS)[i][\"pose\"][\"p\"])\n",
    "\n",
    "Obj = ig_objects.Banana\n",
    "obj = Obj(gym, sim, env)\n",
    "robot.setup_tensors()\n",
    "obj.setup_tensors()\n",
    "\n",
    "gym.get_actor_actuator_count(env, 4)\n",
    "\n",
    "robot.setup_tensors()\n",
    "\n",
    "for i in range(5):\n",
    "    print(gym.get_actor_name(env, i))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7d870-04b0-455f-b1af-1827ec83baf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Force Opt with CvxpyLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ff845d-d7f5-44ae-85eb-3a641b9c16ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:46.500068Z",
     "iopub.status.busy": "2022-06-10T23:30:46.499812Z",
     "iopub.status.idle": "2022-06-10T23:30:46.577963Z",
     "shell.execute_reply": "2022-06-10T23:30:46.577229Z",
     "shell.execute_reply.started": "2022-06-10T23:30:46.500032Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "```python\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "from diffcp import SolverError\n",
    "\n",
    "\n",
    "class ForceOptProblem:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_mu=1.0,\n",
    "        mass=0.0166,\n",
    "        target_n=1.0,\n",
    "        cone_approx=False,\n",
    "        object_frame=False,\n",
    "    ):\n",
    "        self.obj_mu = obj_mu\n",
    "        self.mass = mass\n",
    "        self.target_n = target_n  # 1.0\n",
    "        self.cone_approx = cone_approx\n",
    "        self.object_frame = object_frame\n",
    "        self.setup_cvxpy_layer(target_n, obj_mu, mass)\n",
    "\n",
    "    def setup_cvxpy_layer(self, target_n=1.0, obj_mu=1.0, mass=None):\n",
    "        # Try solving optimization problem\n",
    "        # contact force decision variable\n",
    "        target_n_t = torch.as_tensor(np.array([0, 0, target_n] * 3),\n",
    "                                     dtype=torch.float32)\n",
    "        target_n_cp = cp.Parameter((9,),\n",
    "                                   name=\"target_n\",\n",
    "                                   value=target_n_t.data.numpy())\n",
    "        L = cp.Variable(9, name=\"l\")\n",
    "        W = cp.Parameter((6,), name=\"w_des\")\n",
    "        G = cp.Parameter((6, 9), name=\"grasp_m\")\n",
    "        cm = np.vstack((np.eye(3), np.zeros((3, 3)))) * mass\n",
    "\n",
    "        inputs = [G, W, target_n_cp]\n",
    "        outputs = [L]\n",
    "        # self.Cm = cp.Parameter((6, 3), value=cm*self.mass, name='com')\n",
    "\n",
    "        f_g = np.array([0, 0, -9.81])\n",
    "        if self.object_frame:\n",
    "            R_w_2_o = cp.Parameter((6, 6), name=\"r_w_2_o\")\n",
    "            w_ext = W + R_w_2_o @ cm @ f_g\n",
    "            inputs.append(R_w_2_o)\n",
    "        else:\n",
    "            w_ext = W + cm @ f_g\n",
    "\n",
    "        f = G @ L - w_ext  # generated contact forces must balance wrench\n",
    "\n",
    "        # Objective function - minimize force magnitudes\n",
    "        contact_force = L - target_n_cp\n",
    "        cost = cp.sum_squares(contact_force)\n",
    "\n",
    "        # Friction cone constraints; >= 0\n",
    "        constraints = []\n",
    "        cone_constraints = []\n",
    "        if self.cone_approx:\n",
    "            cone_constraints += [cp.abs(L[1::3]) <= self.obj_mu * L[::3]]\n",
    "            cone_constraints += [cp.abs(L[2::3]) <= self.obj_mu * L[::3]]\n",
    "        else:\n",
    "            cone_constraints.append(\n",
    "                cp.SOC(self.obj_mu * L[::3], (L[2::3] + L[1::3])[None]))\n",
    "        constraints.append(f == np.zeros(f.shape))\n",
    "\n",
    "        self.prob = cp.Problem(cp.Minimize(cost),\n",
    "                               cone_constraints + constraints)\n",
    "        self.policy = CvxpyLayer(self.prob, inputs, outputs)\n",
    "\n",
    "    def balance_force_test(self, des_wrench, balance_force, grasp_points,\n",
    "                           normals, obj_orientation):\n",
    "        if self.object_frame:\n",
    "            R_w_2_o = self.get_w2o_rot(obj_orientation)\n",
    "            weight = (R_w_2_o @ np.vstack(\n",
    "                [np.eye(3) * self.mass, np.zeros(\n",
    "                    (3, 3))]) @ np.array([0, 0, -self.gravity]))\n",
    "        else:\n",
    "            weight = np.vstack([np.eye(3), np.zeros(\n",
    "                (3, 3))]) @ np.array([0, 0, -self.gravity * self.mass])\n",
    "        G = grasp_matrix(grasp_points, normals)\n",
    "        w_ext = des_wrench + weight\n",
    "        f = G @ balance_force - w_ext\n",
    "        return f\n",
    "\n",
    "    def run_fop(self, des_wrench, grasp_points, normals, obj_orientation=None):\n",
    "        G_t = grasp_matrix(\n",
    "            grasp_points.unsqueeze(0).cpu(),\n",
    "            normals.unsqueeze(0).cpu())\n",
    "        des_wrench_t = torch.as_tensor(des_wrench, dtype=torch.float32)\n",
    "        target_n_t = torch.as_tensor(np.array([0, 0, self.target_n] * 3),\n",
    "                                     dtype=torch.float32)\n",
    "        inputs = [G_t, des_wrench_t, target_n_t]\n",
    "        if self.object_frame:\n",
    "            assert (obj_orientation is not None\n",
    "                   ), \"fop requires obj_orientation arg if using object frame\"\n",
    "            R_w_2_o = self.get_w2o_rot(obj_orientation)\n",
    "            R_w_2_o_t = torch.as_tensor(R_w_2_o, dtype=torch.float32)\n",
    "            inputs.append(R_w_2_o_t)\n",
    "        try:\n",
    "            (balance_force,) = self.policy(*inputs)\n",
    "            return balance_force\n",
    "        except SolverError:\n",
    "            return torch.zeros((3, 3), dtype=torch.float32)\n",
    "\n",
    "    def __call__(self, des_wrench, grasp_points, normals, obj_orientation=None):\n",
    "        return self.run_fop(des_wrench, grasp_points, normals, obj_orientation)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_w2o_rot(obj_orientation):\n",
    "        R_w_2_o = Rotation.from_quat(obj_orientation).as_matrix().T\n",
    "        R_w_2_o = block_diag(R_w_2_o, R_w_2_o)\n",
    "        return R_w_2_o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac02c57-17fb-4c25-9173-fd06a20372f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Object Position Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c167b04-eb39-4c6c-b193-4952128aba79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:46.580044Z",
     "iopub.status.busy": "2022-06-10T23:30:46.579824Z",
     "iopub.status.idle": "2022-06-10T23:30:46.625486Z",
     "shell.execute_reply": "2022-06-10T23:30:46.624913Z",
     "shell.execute_reply.started": "2022-06-10T23:30:46.580015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def object_pos_control(\n",
    "    obj,\n",
    "    in_normal,\n",
    "    target_position=None,\n",
    "    target_normal=0.4,\n",
    "    kp=0.4,\n",
    "    kd=0.1,\n",
    "    return_wrench=False,\n",
    "):\n",
    "    \"\"\"Object position control for lifting trajectory\"\"\"\n",
    "    if target_position is None:\n",
    "        target_position = np.array([0.0, 0.0, robot.target_height])\n",
    "    tip_position = robot.position\n",
    "    vel = obj.velocity\n",
    "    angular_vel = obj.angular_velocity\n",
    "    quat = Quaternion.fromWLast(obj.orientation)\n",
    "    target_quat = Quaternion.Identity()\n",
    "    cg_pos = obj.get_CG()  # thoughts it eliminates the pendulum effect? possibly?\n",
    "\n",
    "    pos_error = cg_pos - target_position\n",
    "    object_weight_comp = obj.mass * 9.8 * torch.tensor([0, 0, 1])\n",
    "    # target_force = object_weight_comp - 0.9 * pos_error - 0.4 * vel\n",
    "    # banana tuning\n",
    "    target_force = object_weight_comp - kp * pos_error - kd * vel\n",
    "    target_torque = (-0.04 * (quat @ target_quat.T).to_tangent_space() -\n",
    "                     0.0001 * angular_vel)\n",
    "    if return_wrench:\n",
    "        return torch.cat([target_force, target_torque])\n",
    "    # grasp points in object frame\n",
    "    # TODO: compute tip radius here?\n",
    "    grasp_points = tip_position - cg_pos\n",
    "    in_normal = torch.stack([quat.rotate(x) for x in in_normal], axis=0)\n",
    "    try:\n",
    "        global_forces = calculate_grip_forces(\n",
    "            grasp_points,\n",
    "            in_normal,\n",
    "            target_force,\n",
    "            target_torque,\n",
    "            target_normal,\n",
    "            obj.mu,\n",
    "        )\n",
    "    except AssertionError:\n",
    "        logging.warning(\"solve failed, maintaining previous forces\")\n",
    "        global_forces = (robot.previous_global_forces\n",
    "                        )  # will fail if we failed solve on first iteration\n",
    "        assert global_forces is not None\n",
    "    else:\n",
    "        robot.previous_global_forces = global_forces\n",
    "\n",
    "    return global_forces, target_force, target_torque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784efea-564c-4a8b-9b91-b750c22abee9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-09T06:44:40.724062Z",
     "iopub.status.idle": "2022-06-09T06:44:40.724258Z",
     "shell.execute_reply": "2022-06-09T06:44:40.724160Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load env and object\n",
    "```python\n",
    "Obj = ig_objects.Banana\n",
    "grasp_points = torch.tensor(\n",
    "    [[0.0, 0.05, 0.05], [0.03, -0.05, 0.05], [-0.03, -0.05, 0.05]]\n",
    ")\n",
    "grasp_normals = torch.tensor([[0.0, -1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]])\n",
    "robot_kwargs = dict(grasp_vars=(grasp_points, grasp_normals))\n",
    "\n",
    "tf = TriFingertipEnv(viewer=True, robot=True, Obj=Obj, **robot_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4da17-d37a-4513-b7ba-1dd96f58abd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b941fc-ca6d-4ff0-a662-bfb2c47ab4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:46.627023Z",
     "iopub.status.busy": "2022-06-10T23:30:46.626821Z",
     "iopub.status.idle": "2022-06-10T23:30:51.690262Z",
     "shell.execute_reply": "2022-06-10T23:30:51.689411Z",
     "shell.execute_reply.started": "2022-06-10T23:30:46.626995Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Trainer: ngp | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-10_16-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span> | cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span> | fp32 | \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/scr-ssd/ksrini/nerf_grasping/torch-ngp/logs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">banana</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Trainer: ngp | \u001b[1;36m2022\u001b[0m-\u001b[1;36m06\u001b[0m-10_16-\u001b[1;36m30\u001b[0m-\u001b[1;36m49\u001b[0m | cu\u001b[1;92mda:0\u001b[0m | fp32 | \n",
       "\u001b[35m/scr-ssd/ksrini/nerf_grasping/torch-ngp/logs/\u001b[0m\u001b[95mbanana\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> #parameters: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12667002</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m #parameters: \u001b[1;36m12667002\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Loading latest checkpoint <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Loading latest checkpoint \u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Latest checkpoint is \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/scr-ssd/ksrini/nerf_grasping/torch-ngp/logs/banana/checkpoints/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ngp_ep0200.pth.tar</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Latest checkpoint is \n",
       "\u001b[35m/scr-ssd/ksrini/nerf_grasping/torch-ngp/logs/banana/checkpoints/\u001b[0m\u001b[95mngp_ep0200.pth.tar\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> loaded model.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m loaded model.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>WARN<span style=\"font-weight: bold\">]</span> unexpected keys: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'density_grid'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'step_counter'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mWARN\u001b[1m]\u001b[0m unexpected keys: \u001b[1m[\u001b[0m\u001b[32m'density_grid'\u001b[0m, \u001b[32m'step_counter'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>WARN<span style=\"font-weight: bold\">]</span> Failed to load optimizer, use default.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mWARN\u001b[1m]\u001b[0m Failed to load optimizer, use default.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>WARN<span style=\"font-weight: bold\">]</span> Failed to load scheduler, use default.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mWARN\u001b[1m]\u001b[0m Failed to load scheduler, use default.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> loaded scaler.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m loaded scaler.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_gym(t=450):\n",
    "    for _ in range(t): step_gym()\n",
    "\n",
    "gym = gymapi.acquire_gym()\n",
    "\n",
    "sim = setup_sim()\n",
    "env = setup_env()\n",
    "setup_stage(env)\n",
    "viewer = setup_viewer()\n",
    "\n",
    "Obj = ig_objects.Banana\n",
    "grasp_points, grasp_normals = Obj.grasp_points, Obj.grasp_normals\n",
    "\n",
    "grasp_normals = grasp_normals / grasp_normals.norm(dim=1, keepdim=True)\n",
    "grasp_vars = (grasp_points, grasp_normals)\n",
    "\n",
    "# Creates the robot, fop objective, and object\n",
    "robot = FingertipRobot(gym, sim, env, grasp_vars=grasp_vars, \n",
    "                       norm_start_offset=0.)\n",
    "obj = Obj(gym, sim, env)\n",
    "\n",
    "robot.setup_tensors()\n",
    "obj.setup_tensors()\n",
    "obj.load_nerf_model()\n",
    "obj.load_trimesh()\n",
    "run_gym(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d7ddd-3c17-4041-aa5e-c1b2a9015e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T12:22:58.544214Z",
     "iopub.status.busy": "2022-06-10T12:22:58.543765Z",
     "iopub.status.idle": "2022-06-10T12:22:58.595695Z",
     "shell.execute_reply": "2022-06-10T12:22:58.594996Z",
     "shell.execute_reply.started": "2022-06-10T12:22:58.544156Z"
    },
    "tags": []
   },
   "source": [
    "generates grasp matrix from sampled normals\n",
    "```python\n",
    "nerf_tip_pos = grasp_utils.ig_to_nerf(closest_points)\n",
    "_, grad_ests = grasp_utils.est_grads_vals(\n",
    "                obj.model,\n",
    "                nerf_tip_pos.reshape(1, -1, 3).cuda(),\n",
    "                sigma=5e-3,\n",
    "                num_samples=1000,\n",
    "                method=\"gaussian\",\n",
    "            )\n",
    "grad_ests = grad_ests.reshape(3, 3).float()\n",
    "grad_ests /= grad_ests.norm(dim=1, keepdim=True)\n",
    "grad_ests = grasp_utils.nerf_to_ig(grad_ests.cpu().detach().numpy())\n",
    "G = grasp_matrix(robot.position.cuda().unsqueeze(0), grad_ests.unsqueeze(0))[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9aaa600-4a21-4ab2-bbdb-e4ccc6e3b2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:51.691824Z",
     "iopub.status.busy": "2022-06-10T23:30:51.691583Z",
     "iopub.status.idle": "2022-06-10T23:30:51.722343Z",
     "shell.execute_reply": "2022-06-10T23:30:51.721586Z",
     "shell.execute_reply.started": "2022-06-10T23:30:51.691787Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0977,  0.1510,  0.0000],\n",
      "        [ 0.0739,  0.1781,  0.0000]])\n",
      "tensor([[ 0.0977, -0.1510,  0.0000],\n",
      "        [ 0.1716,  0.0271,  0.0000]])\n",
      "tensor([[-0.0739, -0.1781,  0.0000],\n",
      "        [-0.1716, -0.0271,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(robot.position):\n",
    "    print(np.linalg.norm(p - robot.position[np.where(np.arange(3) != i)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c332ec0-8c5f-4adc-bdd1-1b7e26267c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:19:16.602033Z",
     "iopub.status.busy": "2022-06-10T23:19:16.601618Z",
     "iopub.status.idle": "2022-06-10T23:19:23.800038Z",
     "shell.execute_reply": "2022-06-10T23:19:23.799123Z",
     "shell.execute_reply.started": "2022-06-10T23:19:16.601976Z"
    },
    "tags": []
   },
   "source": [
    "robot.reset_actor()\n",
    "obj.reset_actor()\n",
    "run_gym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19bc33ad-710c-476f-997f-8541cb7e6f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:45:46.197769Z",
     "iopub.status.busy": "2022-06-10T23:45:46.197321Z",
     "iopub.status.idle": "2022-06-10T23:45:46.388374Z",
     "shell.execute_reply": "2022-06-10T23:45:46.387595Z",
     "shell.execute_reply.started": "2022-06-10T23:45:46.197710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "robot.reset_actor(grasp_vars)\n",
    "obj.reset_actor()\n",
    "run_gym(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb949d2-be6f-41e3-90e1-6a428a771815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:27:07.055156Z",
     "iopub.status.busy": "2022-06-10T23:27:07.054706Z",
     "iopub.status.idle": "2022-06-10T23:27:14.266329Z",
     "shell.execute_reply": "2022-06-10T23:27:14.265553Z",
     "shell.execute_reply.started": "2022-06-10T23:27:07.055096Z"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "markers = ig_viz_utils.visualize_markers(gym, env, sim, \n",
    "                                         positions=grasp_points.numpy(),\n",
    "                                         colors=[[0.,1.,0.]]*3)\n",
    "run_gym()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6552da7-3913-47e8-8e29-473cdde8280a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T23:30:58.990834Z",
     "iopub.status.busy": "2022-06-10T23:30:58.990657Z",
     "iopub.status.idle": "2022-06-10T23:30:59.017580Z",
     "shell.execute_reply": "2022-06-10T23:30:59.017040Z",
     "shell.execute_reply.started": "2022-06-10T23:30:58.990808Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0386,  0.0552,  0.0210],\n",
       "         [-0.0238, -0.1467,  0.0225],\n",
       "         [ 0.0859,  0.0824,  0.0039]]),\n",
       " tensor([[-0.0386,  0.0552,  0.0210],\n",
       "         [-0.0238, -0.1467,  0.0225],\n",
       "         [ 0.0859,  0.0824,  0.0231]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grasp_points, robot.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5605c17-de26-4191-88a3-0cc1f55ff5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T00:11:44.713800Z",
     "iopub.status.busy": "2022-06-11T00:11:44.713348Z",
     "iopub.status.idle": "2022-06-11T00:11:50.695873Z",
     "shell.execute_reply": "2022-06-11T00:11:50.695141Z",
     "shell.execute_reply.started": "2022-06-11T00:11:44.713745Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Unable to import pinocchio, skipping import\n",
      "MODE: reach\n",
      "TIMESTEP: 0\n",
      "POSITION ERR: tensor([[ 0.0054, -0.0140, -0.0090],\n",
      "        [ 0.0111,  0.0428, -0.0105],\n",
      "        [-0.0093, -0.0019,  0.0081]])\n",
      "VELOCITY: tensor([[ 2.4024e-02,  7.1553e-02, -3.0990e-09],\n",
      "        [-1.2876e-02, -1.6158e-02,  1.8587e-09],\n",
      "        [ 6.1028e-03, -2.2804e-04, -8.1491e-10]])\n",
      "FORCE MAG: tensor(0.0251)\n",
      "OBJECT FORCES: (-0.03335074, -0.0044632, 0.15330395)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr-ssd/ksrini/nerf_grasping/nerf_grasping/sim/ig_robot.py:916: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(self.grasp_normals * 0.1) + pos_control + vel_control\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: grasp\n",
      "TIMESTEP: 50\n",
      "POSITION ERR: tensor([[-0.0179, -0.0056, -0.0210],\n",
      "        [ 0.0037, -0.0008, -0.0229],\n",
      "        [-0.0002, -0.0012, -0.0093]])\n",
      "VELOCITY: tensor([[-1.8233e-02,  9.8583e-03,  1.9227e-02],\n",
      "        [-2.0041e-02, -8.6857e-02,  2.2425e-02],\n",
      "        [ 1.3381e-03,  3.1084e-04, -5.4886e-11]])\n",
      "FORCE MAG: tensor(0.2796)\n",
      "OBJECT FORCES: (2.1933543e-08, 0., 0.163246)\n",
      "MODE: lift\n",
      "TIMESTEP: 100\n",
      "POSITION ERR: tensor([[ 1.2836e-03, -8.0269e-05, -4.2922e-03],\n",
      "        [ 3.5908e-05,  6.0413e-04, -4.2922e-03],\n",
      "        [-1.5839e-04, -1.0719e-03, -4.2922e-03]])\n",
      "VELOCITY: tensor([[ 1.0206e-04, -1.0634e-04, -1.1571e-11],\n",
      "        [ 3.2687e-02,  1.2954e-01, -4.3802e-09],\n",
      "        [ 2.7639e-04, -8.2391e-05, -1.5752e-10]])\n",
      "FORCE MAG: tensor(1.4006)\n",
      "OBJECT FORCES: (-0.03036795, -0.06409886, 0.15860574)\n",
      "MODE: lift\n",
      "TIMESTEP: 150\n",
      "POSITION ERR: tensor([[-0.0161, -0.0035,  0.0012],\n",
      "        [-0.0012, -0.0061, -0.0015],\n",
      "        [ 0.0021, -0.0147, -0.0072]])\n",
      "VELOCITY: tensor([[-0.0095,  0.0092,  0.0304],\n",
      "        [-0.0059, -0.0183,  0.0479],\n",
      "        [ 0.0384, -0.0202,  0.0457]])\n",
      "FORCE MAG: tensor(1.2719)\n",
      "OBJECT FORCES: (0.20621182, -0.08011664, 0.2371363)\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "f_lift = None\n",
    "\n",
    "for timestep in range(400):\n",
    "    time.sleep(0.01)\n",
    "    step_gym()\n",
    "    robot.control(timestep, obj)\n",
    "    # finds the closest contact points to the original grasp normal + grasp_point ray\n",
    "#     closest_points = ig_utils.closest_point(\n",
    "#         grasp_points, grasp_points + grasp_normals, robot.position\n",
    "#     )\n",
    "#     closest_points[:, 2] = obj.position[2] # + 0.005\n",
    "#     if timestep < 100:\n",
    "#         mode = \"reach\"\n",
    "#         f = robot.position_control(grasp_points, kp=0.2, kd=0.005)\n",
    "#         pos_err = robot.position - grasp_points\n",
    "#     elif timestep < 200:\n",
    "#         mode = \"grasp\"\n",
    "#         pos_err = closest_points - robot.position\n",
    "#         pos_control = pos_err * 3\n",
    "#         f = torch.tensor(grasp_normals * 0.02) + pos_control - .005*robot.velocity\n",
    "#     else:\n",
    "#         mode = \"lift\"\n",
    "#         pos_err = closest_points - robot.position\n",
    "#         height_err = robot.target_height - obj.position[-1]\n",
    "\n",
    "#         # f, target_force, target_torque = object_pos_control(\n",
    "#         #     obj, grasp_normals, target_normal=0.15\n",
    "#         gp, gn = get_mesh_contacts(obj.gt_mesh, closest_points)\n",
    "#         gn_obj = torch.tensor(gn, dtype=torch.float32)\n",
    "#         gn_world = torch.stack([\n",
    "#             Quaternion.fromWLast(obj.orientation).rotate(x) for x in gn_obj])\n",
    "#         f_lift, target_force, target_torque = object_pos_control(\n",
    "#             obj, ge, target_normal=0.4, kp=0.5, kd=0.1\n",
    "#         )\n",
    "#         des_wrench = torch.cat([target_force, target_torque])\n",
    "#         # des_wrench = torch.tensor([0, 0, 0.1, 0, 0, 0])\n",
    "#         # f_lift = fop_obj(des_wrench, closest_points, ge)\n",
    "#         f = f_lift.reshape((3, 3))  # + pos_err * 3\n",
    "\n",
    "#     gym.refresh_net_contact_force_tensor(sim)\n",
    "#     robot.apply_fingertip_forces(f)\n",
    "#     if timestep >= 100 and timestep % 50 == 0:\n",
    "#         print(\"MODE:\", mode)\n",
    "#         print(\"TIMESTEP:\", timestep)\n",
    "#         print(\"POSITION ERR:\", pos_err)\n",
    "#         print(\"VELOCITY:\", robot.velocity)\n",
    "#         print(\"FORCE MAG:\", f.norm())\n",
    "#         print(\"Z Force:\", f[:, 2])\n",
    "#         print(\"OBJECT FORCES:\", gym.get_rigid_contact_forces(sim)[obj.actor])\n",
    "    if (robot.position[:, -1] >= 0.1).any():\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
